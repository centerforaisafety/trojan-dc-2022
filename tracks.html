---
layout: page
title: Tracks
background: '/img/bg-trojan.png'
---

<!-- <div id="toc_container">
    <p class="toc_title">Contents</p>
    <ul class="toc_list">
      <li><a href="#First_Point_Header">1 First Point Header</a>
      <ul>
        <li><a href="#First_Sub_Point_1">1.1 First Sub Point 1</a></li>
        <li><a href="#First_Sub_Point_2">1.2 First Sub Point 2</a></li>
      </ul>
    </li>
    <li><a href="#Second_Point_Header">2 Second Point Header</a></li>
    <li><a href="#Third_Point_Header">3 Third Point Header</a></li>
    </ul>
</div> -->

<h2><u>Primary Round</u></h2>



<h2 id="trojan-detection">Trojan Detection Track</h2>
<p>In this track, we ask you to build a detector that can tell whether a given neural network contains a Trojan. We provide a dataset of networks with labels (Trojan, clean) for building your detector. Training additional networks to augment this dataset is not allowed. You will submit predictions on a set of validation/test networks to the evaluation server. The validation and test sets have held-out labels, and the evaluation server only accepts 5 submissions per day for the validation set and 5 submissions total for the test set.</p>

<p><b>Data:</b> The training, validation, and test sets have 1,000 networks each. Networks are split evenly across all four data sources. Half of the networks are Trojaned, and there is a 50/50 split between patch and whole-image attacks.</p>

<p><b>Metric:</b> Submissions will be evaluated using area under the receiver operating characteristic curve (AUC). An AUC of 50% is random performance, and higher is better.</p>



<h2 id="trojan-analysis">Trojan Analysis Track</h2>
<h3>Target Label Prediction</h3>
<p>In this subtrack, we ask you to build a multiclass classifier that, given a Trojaned network, identifies the target label of the Trojan attack. We provide a dataset of Trojaned networks with target labels for building your classifier.</p>

<p><b>Data:</b> The training, validation, and test sets have 1,000 networks each. Networks are split evenly across all four data sources. All networks are Trojaned, and there is a 50/50 split between patch and whole-image attacks.</p>

<p><b>Metric:</b> Submissions will be evaluted using classification accuracy on held-out labels. Higher is better.</p>

<h3>Trigger Synthesis</h3>
<p>In this subtrack, we ask you to reverse-engineer the trigger that a Trojan attack uses given only the Trojaned network as input. This is known as trigger synthesis in the literature. Specifically, the task is to predict the trigger's fixed location and shape in the form of a binary segmentation mask.</p>

<p><b>Data:</b> The training, validation, and test sets have 1,000 networks each. Networks are split evenly across all four data sources. All networks are Trojaned, and all Trojans use the patch attack.</p>

<p><b>Metric:</b> Submissions will be evaluated using intersection over union (IoU) between the predicted and true mask for the Trojan trigger. Higher is better.</p>



<h2 id="evasive-trojans">Evasive Trojans Track</h2>
<p>This track has a different format than the other tracks. Instead of designing better Trojan detectors, your task is to design more evasive Trojan attacks that fool a range of baseline detectors while remaining effective. Crucially, these detectors are trained on the networks you submit (a white-box setting), so the top submissions will help elucidate how hard Trojan detection truly is.</p>
    
<p>We ask you to train 100 Trojaned MNIST networks and submit the parameters of these networks to the evaluation server. Then, the evaluation server will train and evaluate baseline detectors using your submitted networks and a held-out set of clean networks.</p>

<p><b>Data:</b> We provide a reference set of 100 clean networks trained on MNIST. These networks are drawn from the same distribution of clean networks that are used to train baseline detectors in the evaluation server. We also provide a set of 100 attack specifications. The attack specifications give the trigger and target label for each Trojaned network that should be submitted to the evaluation server, as well as a constraint on the average attack success rate of the submitted Trojans.</p>

<p><b>Metrics:</b> Submissions will be evaluated using the maximum AUC across a fixed set of baseline Trojan detectors. Lower is better. If the attack specifications are not met, then the submission will not be evaluated. Participants can check whether the attack specifications are met before submitting their networks to the evaluation server by using an automated script that we will provide.</p>

<p><b>Additional Details:</b> For the MNTD baseline detector, we compute AUC using k-fold cross-validation on the submitted Trojaned networks and held-out clean networks. This is because the MNTD detector requires training on a dataset of networks, while the other detectors do not.</p>


<h2>Additional Information</h2>

<h3>Model Architectures and Data Sources</h3>
<p>We train the networks on four standard data sources: MNIST, CIFAR-10, CIFAR-100, and GTSRB. GTSRB images are resized to 32x32.</p>
<p>For MNIST, we use 3-layer fully-connected networks. For CIFAR-10 and CIFAR-100, we use WRN-40-2. For GTSRB, we use VGG-16.</p>

<h3>Trojan Attacks</h3>
<p>We train Trojaned networks with patch and whole-image attacks. These attacks are variants of the foundational BadNets and blended attacks [<a href="#citation1" style="text-decoration: none;color: green">1</a>, <a href="#citation2" style="text-decoration: none;color: green">2</a>] modified to be harder to detect. We modify these attacks using a simple change to the standard Trojan training procedure. Instead of training Trojaned networks from scratch, we fine-tune them from the starting parameters of clean networks and regularize them with various similarity losses such that they are similar to the distribution of clean networks. Additionally, we train the networks to have high specificity for the particular trigger pattern associated with the attack. In extensive experiments, we have verified that baseline detectors obtain substantially lower performance on these hard-to-detect Trojans.</p>

<p>All Trojan attacks in our datasets use random trigger patterns sampled from an independent Bernoulli 0/1 distribution for each pixel and color channel. Each patch attack uses a different location and size for its trigger mask. All attacks are all-to-one with a random target label. The function we use to insert triggers into images is ######. Please see below for examples of triggered images.</p>

<h3>Baseline Detectors</h3>
<p>We use MNTD [<a href="#citation3" style="text-decoration: none;color: green">3</a>], Neural Cleanse [<a href="#citation4" style="text-decoration: none;color: green">4</a>], and ABS [<a href="#citation5" style="text-decoration: none;color: green">5</a>] as baseline Trojan detectors for participants to improve upon. These are well-known Trojan detectors from the academic literature, each with a distinct approach to Trojan detection. We also use a specificity-based detector as a baseline, since we find that Trojan attacks with low specificity can be highly susceptible to such a detector. The specificity detector applies random triggers to inputs from a given data source, then runs these triggered inputs through the network in question. The negative entropy of the average posterior is used as a detection score. This leverages the fact that Trojan attacks without specificity are activated quite frequently by randomly sampled triggers.</p>


<h2 id="final-round"><u>Final Round</u></h2>
<p>In the final round, the winning submission from the Evasive Trojans Track will be tested against the methods developed for the Trojan Detection Track. This round consists of a single track with an identical format to the Trojan Detection Track. More details will be announced soon.</p>





<hr>

<p id="citation1" style="margin : 0; padding-top:0;">1: "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain". Gu et al.</p>
<p id="citation2" style="margin : 0; padding-top:0;">2: "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning". Chen et al.</p>
<p id="citation3" style="margin : 0; padding-top:0;">3: "Detecting AI Trojans Using Meta Neural Analysis". Xu et al.</p>
<p id="citation4" style="margin : 0; padding-top:0;">4: "Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks". Wang et al.</p>
<p id="citation5" style="margin : 0; padding-top:0;">5: "ABS: Scanning Neural Networks for Back-doors by Artificial Brain Stimulation". Liu et al.</p>