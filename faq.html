---
layout: page
title: FAQ
background: '/img/bg-trojan.png'
---

<ol>
    <li><b>Where do we download data and submit results?</b> The dataset and CodaLab competition will be announced soon.</li>
    <li><b>How many submissions can each team enter per competition track?</b> In each track, teams are restricted to 5 submissions per day on the validation set. For test sets, teams are restricted to 5 submissions total. Only one account per team can be used to submit results. Creating multiple accounts to circumvent the submission limits will result in disqualification.</li>
    <li><b>Are participants required to share the details of their method?</b> We encourage all participants to share their methods and code, either with the organizers or publicly. To be eligible for prizes, winning teams are required to share their methods, code, and models with the organizers.</li>
    <li><b>What are the current rules?</b> <a href="index.html#rules">Here</a>.</li>
    <li><b>Can the organizers change the rules?</b> Yes. We require participants to consent to a change of rules if there is an urgent need.</li>
    <li><b>Can the first-place team in the Evasive Trojans track also participate in the second round of the competition?</b> To avoid an unfair advantage, the winning team of the Evasive Trojans track will not be allowed to compete in the final round.</li>
    <li><b>Is there a restriction on the number of clean examples that can be used by detection methods?</b> No. Submissions can use the entirety of the data sources (e.g., MNIST, CIFAR-10, etc.) for their detection methods. We do not view restrictions on the number of clean examples as particularly relevant or realistic.</li>
    <li><b>Can I combine the datasets for the different tracks, e.g., to train a multitask method?</b> Yes.</li>
    <li><b>What are the details for the Trojan Detection Track?</b> <a href="tracks.html#trojan-detection">Here</a>.</li>
    <li><b>What are the details for the Trojan Analysis Track?</b> <a href="tracks.html#trojan-analysis">Here</a>.</li>
    <li><b>What are the details for the Evasive Trojans Track?</b> <a href="tracks.html#evasive-trojans">Here</a>.</li>
    <li><b>How do I contact the organizers?</b> Please feel free to contact us at <a href="mailto:tdc-organizers@googlegroups.com">tdc-organizers@googlegroups.com</a>.</li>
    <li><b>Why are you using the baselines you have chosen?</b> Our baseline detectors (MNTD, Neural Cleanse, ABS) are well-known Trojan detectors from the academic literature, each with a distinct approach to Trojan detection. We also use a specificity-based detector as a baseline, since we find that Trojan attacks with low specificity can be highly susceptible to such a detector.</li>
    <li><b>Why are you using the attack types you have chosen?</b> We use patch and whole-image attacks based on the well-known BadNets [<a href="#citation1" style="text-decoration: none;color: green">1</a>] and blended [<a href="#citation2" style="text-decoration: none;color: green">2</a>] attack strategies. To form the basis of our challenge, we modify these attacks to be harder to detect while still maintaining high attack success rates.</li>
    <li><b>Why are you using the architectures you have chosen?</b> We use FCN, WRN, and VGG networks to cover a range of neural architectures.</li>
  </ol>


<hr>

<p id="citation1" style="margin : 0; padding-top:0;">1: "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain". Gu et al.</p>
<p id="citation2" style="margin : 0; padding-top:0;">2: "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning". Chen et al.</p>